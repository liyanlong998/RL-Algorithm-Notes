{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc20db0",
   "metadata": {},
   "source": [
    "# 1.定义模型（训练与测试）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from collections import defaultdict  #创建默认字典\n",
    "\n",
    "#定义一个智能体的类\n",
    "class Qlearning(object):\n",
    "    def __init__(self,cfg):\n",
    "        '''智能体类\n",
    "        Args:\n",
    "            cfg (class): 超参数类\n",
    "        '''\n",
    "        #动作空间大小（可执行的动作数量）\n",
    "        self.n_actions = cfg.n_actions    \n",
    "        #探索策略如 ε-greedy ，boltzmann ，softmax， ucb 等\n",
    "        self.exploration_type = 'e-greedy' \n",
    "        #学习率（控制Q值更新的幅度）\n",
    "        self.lr = cfg.lr  \n",
    "        #折扣因子（未来奖励的衰减系数）                  \n",
    "        self.gamma = cfg.gamma  \n",
    "        #当前ε值（初始值为超参数中的起始ε）           \n",
    "        self.epsilon = cfg.epsilon_start   \n",
    "        #记录采样次数（用于ε的衰减）\n",
    "        self.sample_count = 0        \n",
    "        # ε的初始值      \n",
    "        self.epsilon_start = cfg.epsilon_start  \n",
    "        # ε的最终值（衰减下限）\n",
    "        self.epsilon_end = cfg.epsilon_end      \n",
    "        # ε的衰减率（控制衰减速度）\n",
    "        self.epsilon_decay = cfg.epsilon_decay  \n",
    "        # 初始化Q表：用默认字典存储，当遇见未知状态，会自动创建一个条目，不需要初始化所有可能的状态\n",
    "        # 动作空间有4个值，每一个状态都对应4个值，初始都是0\n",
    "        self.Q_table  = defaultdict(lambda: np.zeros(self.n_actions))  \n",
    "\n",
    "\n",
    "    #训练阶段选择动作（包含探索）\n",
    "    def sample_action(self, state):        \n",
    "        ''' 以 e-greedy 策略训练时选择动作 \n",
    "        Args:\n",
    "            state (array): 状态\n",
    "        Returns:\n",
    "            action (int): 动作\n",
    "        ''' \n",
    "        #如果使用贪心策略\n",
    "        if self.exploration_type == 'e-greedy':  \n",
    "            #调用对应的动作选择方法                   \n",
    "            action = self._epsilon_greedy_sample_action(state)      \n",
    "        else:\n",
    "            #否则抛出异常\n",
    "            raise NotImplementedError                               \n",
    "        return action\n",
    "    \n",
    "\n",
    "    #测试阶段选择动作（无探索，纯利用）\n",
    "    def predict_action(self,state):        \n",
    "        ''' 预测动作\n",
    "        Args:\n",
    "            state (array): 状态\n",
    "        Returns:\n",
    "            action (int): 动作\n",
    "        '''\n",
    "        #如果使用贪心策略\n",
    "        if self.exploration_type == 'e-greedy':\n",
    "            #调用对应的动作选择方法\n",
    "            action = self._epsilon_greedy_predict_action(state)\n",
    "        else:\n",
    "            #否则抛出异常\n",
    "            raise NotImplementedError\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    #实现贪婪策略的动作选择（训练用）\n",
    "    def _epsilon_greedy_sample_action(self, state):\n",
    "        ''' \n",
    "        采用 epsilon-greedy 策略进行动作选择 \n",
    "        Args: \n",
    "            state (array): 状态\n",
    "        Returns: \n",
    "            action (int): 动作\n",
    "        ''' \n",
    "        #采样次数+1（用于更新ε）\n",
    "        self.sample_count += 1\n",
    "        #计算当前ε值：指数衰减（随采样次数增加，ε从start衰减到end）\n",
    "        self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "            math.exp(-1. * self.sample_count / self.epsilon_decay) \n",
    "        #以(1-ε)的概率选择当前Q值最大的动作（利用），以ε的概率随机选择动作（探索）\n",
    "        #随机数落在[a, 1]区间的概率正好是1-a,随机数落在 [0, a] 区间的概率正好是a\n",
    "        if np.random.uniform(0, 1) > self.epsilon:\n",
    "            #从Q表中获取当前状态（转为字符串作为键）对应的所有动作的Q值\n",
    "            #返回数组中最大值的索引（即Q值最大的动作编号）\n",
    "            action = np.argmax(self.Q_table[str(state)]) \n",
    "        #随机数小于ε：探索\n",
    "        else:\n",
    "            #从动作空间中随机选一个动作\n",
    "            action = np.random.choice(self.n_actions) \n",
    "        return action\n",
    "    \n",
    "    #测试阶段\n",
    "    def _epsilon_greedy_predict_action(self,state):\n",
    "        ''' \n",
    "        使用 epsilon-greedy 算法进行动作预测 \n",
    "        Args: \n",
    "            state (array): 状态\n",
    "        Returns: \n",
    "            action (int): 动作 \n",
    "        ''' \n",
    "        #找到当前状态中，4个动作最大的Q值，然后返回其索引值\n",
    "        action = np.argmax(self.Q_table[str(state)])\n",
    "        return action\n",
    "    \n",
    "    #Q值表的更新\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        ''' 更新模型\n",
    "        Args:\n",
    "            state (array): 当前状态 \n",
    "            action (int): 当前动作 \n",
    "            reward (float): 当前奖励信号 \n",
    "            next_state (array): 下一个状态 \n",
    "            done (bool): 表示是否达到终止状态 \n",
    "        '''\n",
    "        #取当前状态当前动作的Q值。[action]是一个索引\n",
    "        Q_predict = self.Q_table[str(state)][action] \n",
    "        #如果状态终止，目标Q值就等于当前获得的奖励。状态终止奖励为0\n",
    "        if done: \n",
    "            #所以目标Q值是0\n",
    "            Q_target = reward  \n",
    "        else:\n",
    "            #如果非终止态，用这个公式求目标Q值\n",
    "            Q_target = reward + self.gamma * np.max(self.Q_table[str(next_state)]) \n",
    "        #得到目标Q值之后，更新当前状态当前动作的Q值\n",
    "        self.Q_table[str(state)][action] += self.lr * (Q_target - Q_predict)\n",
    "\n",
    "    def save_model(self,path):\n",
    "        '''\n",
    "        保存模型\n",
    "        Args:\n",
    "            path (str): 模型存储路径 \n",
    "        '''\n",
    "        #序列化储存\n",
    "        import dill\n",
    "        #路径处理库\n",
    "        from pathlib import Path\n",
    "        # 确保存储路径存在 \n",
    "        # 使用path对象创建目录（如果不存在）\n",
    "        # parents=True: 创建所有必要的父目录\n",
    "        # exist_ok=True: 如果目录已存在不会抛出异常\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        #保存模型\n",
    "        torch.save(\n",
    "            #保存的对象，这里是Q表\n",
    "            obj=self.Q_table,\n",
    "            #保存的路径和文件名\n",
    "            f=path+\"Qleaning_model.pkl\",\n",
    "            #指定使用dill而不是默认的pickle进行序列化\n",
    "            pickle_module=dill\n",
    "        )\n",
    "        print(\"Model saved!\")\n",
    "        \n",
    "        \n",
    "    def load_model(self, path):\n",
    "        '''\n",
    "        根据模型路径导入模型\n",
    "        Args:\n",
    "            fpath (str): 模型路径\n",
    "        '''\n",
    "        import dill\n",
    "        self.Q_table =torch.load(f=path+'Qleaning_model.pkl',pickle_module=dill)\n",
    "        print(\"Mode loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17704591",
   "metadata": {},
   "source": [
    "# 2.可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a54a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import time \n",
    "\n",
    "class CliffWalkingVisualizer:\n",
    "    def __init__(self):\n",
    "        # 初始化Pygame\n",
    "        pygame.init()\n",
    "        \n",
    "        # 网格参数\n",
    "        self.GRID_ROWS = 4\n",
    "        self.GRID_COLS = 12\n",
    "        self.CELL_SIZE = 80  # 每个网格单元格的大小（像素）\n",
    "        self.MARGIN = 2      # 网格线宽度\n",
    "        \n",
    "        # 计算窗口大小\n",
    "        self.WINDOW_WIDTH = self.GRID_COLS * (self.CELL_SIZE + self.MARGIN) + self.MARGIN\n",
    "        self.WINDOW_HEIGHT = self.GRID_ROWS * (self.CELL_SIZE + self.MARGIN) + self.MARGIN\n",
    "        \n",
    "        # 颜色定义\n",
    "        self.WHITE = (255, 255, 255)\n",
    "        self.BLACK = (0, 0, 0)\n",
    "        self.GREEN = (0, 200, 0)    # 起点\n",
    "        self.RED = (200, 0, 0)      # 终点\n",
    "        self.ORANGE = (255, 165, 0) # 悬崖\n",
    "        self.BLUE = (100, 100, 255) # 普通路径\n",
    "        self.GRAY = (200, 200, 200) # 网格线\n",
    "        self.AGENT_COLOR = (255, 255, 0)  # 智能体颜色\n",
    "        \n",
    "        # 创建窗口\n",
    "        self.screen = pygame.display.set_mode((self.WINDOW_WIDTH, self.WINDOW_HEIGHT))\n",
    "        pygame.display.set_caption(\"悬崖寻路可视化\")\n",
    "        \n",
    "        # 设置字体\n",
    "        self.font = pygame.font.SysFont('SimHei', 20)  # 使用支持中文的字体\n",
    "        \n",
    "        # 当前智能体位置\n",
    "        self.agent_pos = None\n",
    "    \n",
    "    def update_agent_position(self, state):\n",
    "        \"\"\"更新智能体位置\"\"\"\n",
    "        # 将状态转换为网格坐标\n",
    "        row = state // self.GRID_COLS\n",
    "        col = state % self.GRID_COLS\n",
    "        self.agent_pos = (row, col)\n",
    "    \n",
    "    def render(self, episode, step, reward, total_reward):\n",
    "        \"\"\"渲染当前状态\"\"\"\n",
    "        # 处理事件\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                sys.exit()\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    pygame.quit()\n",
    "                    sys.exit()\n",
    "        \n",
    "        # 填充背景\n",
    "        self.screen.fill(self.GRAY)\n",
    "        \n",
    "        # 绘制网格\n",
    "        for row in range(self.GRID_ROWS):\n",
    "            for col in range(self.GRID_COLS):\n",
    "                # 计算单元格位置\n",
    "                x = col * (self.CELL_SIZE + self.MARGIN) + self.MARGIN\n",
    "                y = row * (self.CELL_SIZE + self.MARGIN) + self.MARGIN\n",
    "                \n",
    "                # 确定单元格颜色\n",
    "                if row == self.GRID_ROWS - 1:  # 最后一行\n",
    "                    if col == 0:  # 起点\n",
    "                        color = self.GREEN\n",
    "                    elif col == self.GRID_COLS - 1:  # 终点\n",
    "                        color = self.RED\n",
    "                    else:  # 悬崖\n",
    "                        color = self.ORANGE\n",
    "                else:  # 其他行\n",
    "                    color = self.BLUE\n",
    "                \n",
    "                # 绘制单元格\n",
    "                pygame.draw.rect(self.screen, color, (x, y, self.CELL_SIZE, self.CELL_SIZE))\n",
    "        \n",
    "        # 绘制智能体\n",
    "        if self.agent_pos:\n",
    "            row, col = self.agent_pos\n",
    "            x = col * (self.CELL_SIZE + self.MARGIN) + self.MARGIN\n",
    "            y = row * (self.CELL_SIZE + self.MARGIN) + self.MARGIN\n",
    "            center_x = x + self.CELL_SIZE // 2\n",
    "            center_y = y + self.CELL_SIZE // 2\n",
    "            pygame.draw.circle(self.screen, self.AGENT_COLOR, (center_x, center_y), self.CELL_SIZE // 3)\n",
    "        \n",
    "        # 添加起点和终点文字标签（居中）\n",
    "        start_text = self.font.render(\"起点\", True, self.BLACK)\n",
    "        start_x = self.MARGIN + (self.CELL_SIZE - start_text.get_width()) / 2\n",
    "        start_y = (self.GRID_ROWS - 1) * (self.CELL_SIZE + self.MARGIN) + self.MARGIN + (self.CELL_SIZE - start_text.get_height()) / 2\n",
    "        self.screen.blit(start_text, (start_x, start_y))\n",
    "        \n",
    "        end_text = self.font.render(\"终点\", True, self.BLACK)\n",
    "        end_x = (self.GRID_COLS - 1) * (self.CELL_SIZE + self.MARGIN) + self.MARGIN + (self.CELL_SIZE - end_text.get_width()) / 2\n",
    "        end_y = (self.GRID_ROWS - 1) * (self.CELL_SIZE + self.MARGIN) + self.MARGIN + (self.CELL_SIZE - end_text.get_height()) / 2\n",
    "        self.screen.blit(end_text, (end_x, end_y))\n",
    "        \n",
    "        # 添加信息显示\n",
    "        info_text = self.font.render(f\"回合: {episode} 步数: {step} 当前奖励: {reward} 总奖励: {total_reward}\", True, self.BLACK)\n",
    "        self.screen.blit(info_text, (10, 10))\n",
    "        \n",
    "        # 更新显示\n",
    "        pygame.display.flip()\n",
    "        \n",
    "        # 控制渲染速度\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"关闭可视化\"\"\"\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e481edb",
   "metadata": {},
   "source": [
    "# 3.定义模型的训练与测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622155c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg, env, agent):\n",
    "    ''' 训练\n",
    "    '''\n",
    "    print(\"开始训练！\")\n",
    "    #记录所有回合的奖励\n",
    "    rewards = []  \n",
    "    #记录所有回合步数\n",
    "    steps = []\n",
    "    #开始训练，回合数=cfg.train_eps\n",
    "    for i_ep in range(cfg.train_eps):\n",
    "        #初始化当前回合的累积奖励\n",
    "        ep_reward = 0  \n",
    "        #初始化当前回合的步数 \n",
    "        ep_step = 0\n",
    "        #重置环境并获取初始状态，设置随机种子\n",
    "        state = env.reset(seed = cfg.seed)  \n",
    "        #当前回合内，步数=cfg.max_steps\n",
    "        for _ in range(cfg.max_steps):\n",
    "            #当前回合步数+1\n",
    "            ep_step += 1\n",
    "            #根据当前状态采样一个动作（训练阶段包含探索）\n",
    "            action = agent.sample_action(state)  \n",
    "            #根据配置决定使用新API还是旧API\n",
    "            if cfg.new_step_api:\n",
    "                #使用 OpenAI Gym 的 new_step_api\n",
    "                #更新环境并返回新状态、奖励、终止状态、截断标志和其他信息\n",
    "                next_state, reward, terminated, truncated , info = env.step(action)  \n",
    "            else:\n",
    "                #使用 OpenAI Gym 的 old_step_api\n",
    "                #更新环境并返回新状态、奖励、终止状态和其他信息\n",
    "                next_state, reward, terminated, info = env.step(action)  \n",
    "            #更新智能体 \n",
    "            agent.update(state, action, reward, next_state, terminated)  \n",
    "            #更新状态\n",
    "            state = next_state  \n",
    "            #增加奖励\n",
    "            ep_reward += reward  \n",
    "            #如果到达终止状态，提前结束当前回合\n",
    "            if terminated:\n",
    "                break\n",
    "        #记录当前回合的步数和总奖励    \n",
    "        steps.append(ep_step)\n",
    "        rewards.append(ep_reward)\n",
    "        #每10个回合打印一次训练进度\n",
    "        if (i_ep + 1) % 10 == 0:\n",
    "            print(f\"回合：{i_ep+1}/{cfg.train_eps}，奖励：{ep_reward:.2f}\")\n",
    "    print(\"完成训练！\")\n",
    "    #返回一个包含所有回合奖励的字典\n",
    "    return {'rewards':rewards}\n",
    "\n",
    "\n",
    "def test(cfg, env, agent):\n",
    "    print(\"开始测试！\")\n",
    "    # 记录所有回合的奖励\n",
    "    # 初始化可视化\n",
    "    visualizer = CliffWalkingVisualizer()\n",
    "    rewards = []  \n",
    "    steps = []\n",
    "    for i_ep in range(cfg.test_eps):\n",
    "        # 一轮的累计奖励 \n",
    "        ep_reward = 0  \n",
    "        ep_step = 0\n",
    "        # 重置环境并获取初始状态\n",
    "        state = env.reset(seed = cfg.seed)   \n",
    "        for _ in range(cfg.max_steps):\n",
    "            if cfg.render:\n",
    "                env.render()\n",
    "            ep_step += 1\n",
    "            action = agent.predict_action(state)  \n",
    "            next_state, reward, terminated, truncated , info = env.step(action)\n",
    "            # 更新状态\n",
    "            state = next_state   \n",
    "            # 增加奖励\n",
    "            ep_reward += reward  \n",
    "            if terminated:\n",
    "                break\n",
    "        steps.append(ep_step)\n",
    "        rewards.append(ep_reward)\n",
    "        print(f\"回合：{i_ep+1}/{cfg.test_eps}，奖励：{ep_reward:.2f}\")\n",
    "    print(\"完成测试\")\n",
    "    env.close()\n",
    "    return {'rewards':rewards}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbfc4f9",
   "metadata": {},
   "source": [
    "# 4.定义环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb6604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "import random\n",
    "def all_seed(env,seed = 1):\n",
    "    ''' 万能的seed函数\n",
    "    '''\n",
    "    #控制环境本身的随机性\n",
    "    env.reset(seed=seed) \n",
    "    #控制所有使用numpy的随机操作\n",
    "    np.random.seed(seed)\n",
    "    #控制python内置模块random的随机性\n",
    "    random.seed(seed)\n",
    "    #控制PyTorch在CPU上的所有随机操作\n",
    "    torch.manual_seed(seed) \n",
    "    #控制PyTorch在GPU上的所有随机操作\n",
    "    torch.cuda.manual_seed(seed) \n",
    "    #控制控制Python的哈希函数行为\n",
    "    #哈希函数在每次解释器启动时会被随机化。这会导致诸如set或dict这类数据结构的迭代顺序每次运行都不同\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) \n",
    "    # 设置CuDNN使用确定性算法，防止底层CUDA库的随机性和不确定性\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    # 闭CuDNN的基准模式，确保确定性\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    #禁用CuDNN，只使用PyTorch自己的CUDA实现\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "#定义环境与智能体配置参数    \n",
    "def env_agent_config(cfg):\n",
    "    #创建指定名称的环境\n",
    "    env = gym.make(cfg.env_name) \n",
    "    #调用上面定义的all_seed函数，设置所有随机种子\n",
    "    all_seed(env,seed=cfg.seed)\n",
    "    #打印观察空间的形状（对于离散空间可能不适用）\n",
    "    print(env.observation_space.shape)\n",
    "    #获取离散状态空间的大小（仅适用于离散观察空间）\n",
    "    n_states = env.observation_space.n\n",
    "    #获取离散动作空间的大小\n",
    "    n_actions = env.action_space.n\n",
    "    print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}\")\n",
    "    #使用setattr将状态空间大小添加到配置对象中，状态空间值=48\n",
    "    setattr(cfg, 'n_states', n_states)\n",
    "    #将动作空间大小添加到配置对象中，动作空间值=4\n",
    "    setattr(cfg, 'n_actions', n_actions) \n",
    "    #将环境的动作空间对象添加到配置中，动作空间对象=Discrete(4)\n",
    "    #动作空间对象提供了更完整的动作空间定义，包含动作数量、动作空间类型、可能的动作值范围、采样方法等\n",
    "    setattr(cfg, 'action_space', env.action_space) \n",
    "    agent = Qlearning(cfg)\n",
    "    return env,agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a0c13",
   "metadata": {},
   "source": [
    "# 5.设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17155565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "class Config:\n",
    "    def __init__(self) -> None:\n",
    "        ## 通用参数\n",
    "        #环境名称\n",
    "        self.env_name = \"CliffWalking-v1\" \n",
    "        #使用新的API\n",
    "        self.new_step_api = True \n",
    "        #不使用任何Wrapper\n",
    "        self.wrapper = None \n",
    "        #不渲染环境\n",
    "        self.render = False \n",
    "        #渲染模式\n",
    "        self.render_mode = \"human\" \n",
    "        #算法名称\n",
    "        self.algo_name = \"Qlearning\" \n",
    "        #运行模式\n",
    "        self.mode = \"train\"\n",
    "        #多线程框架\n",
    "        self.mp_backend = \"mp\" \n",
    "        #随机种子的设置，保证实验结果可复现\n",
    "        self.seed = 1 \n",
    "        #计算设备\n",
    "        self.device = \"cuda\" \n",
    "        #训练回合数\n",
    "        self.train_eps = 500 \n",
    "        #测试回合数\n",
    "        self.test_eps = 10 \n",
    "        #训练期间每隔10个回合评估一次\n",
    "        self.eval_eps = 10 \n",
    "        #每个回合中，每走5步评估一次\n",
    "        self.eval_per_episode = 5 \n",
    "        #单个回合最大步数\n",
    "        self.max_steps = 1000 \n",
    "        #是否从加载模型继续训练或者直接测试，false表示直接训练\n",
    "        self.load_checkpoint = False\n",
    "        #加载模型的文件路径\n",
    "        self.load_path = \"tasks\" \n",
    "        #是否在程序运行时，显示结果图表\n",
    "        self.show_fig = False \n",
    "        #是否将结果保存为图片\n",
    "        self.save_fig = True \n",
    "\n",
    "        ## Qlearing参数\n",
    "        self.epsilon_start = 0.95 # epsilon 初始值\n",
    "        self.epsilon_end = 0.01 # epsilon 终止值\n",
    "        self.epsilon_decay = 300 # epsilon 衰减率\n",
    "        self.gamma = 0.90 # 奖励折扣因子\n",
    "        self.lr = 0.1 # 学习率\n",
    "\n",
    "def smooth(data, weight=0.9):  \n",
    "    '''用于平滑曲线,类似于Tensorboard中的smooth曲线\n",
    "    '''\n",
    "    last = data[0] \n",
    "    smoothed = []\n",
    "    for point in data:\n",
    "        smoothed_val = last * weight + (1 - weight) * point  # 计算平滑值\n",
    "        smoothed.append(smoothed_val)                    \n",
    "        last = smoothed_val                                \n",
    "    return smoothed\n",
    "\n",
    "def plot_rewards(rewards,title=\"learning curve\"):\n",
    "    sns.set()\n",
    "    plt.figure()  # 创建一个图形实例，方便同时多画几个图\n",
    "    plt.title(f\"{title}\")\n",
    "    plt.xlim(0, len(rewards))  # 设置x轴的范围\n",
    "    plt.xticks(np.arange(0, len(rewards)+1, 50))\n",
    "    plt.xlabel('episodes')\n",
    "    plt.plot(rewards, label='rewards')\n",
    "    plt.plot(smooth(rewards), label='smoothed')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042592f6",
   "metadata": {},
   "source": [
    "# 6.开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bf2efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取参数\n",
    "cfg = Config() \n",
    "# 训练\n",
    "env, agent = env_agent_config(cfg)\n",
    "res_dic = train(cfg, env, agent)\n",
    "plot_rewards(res_dic['rewards'], title=f\"training curve on {cfg.device} of {cfg.algo_name} for {cfg.env_name}\")\n",
    "\n",
    "# 测试\n",
    "res_dic = test(cfg, env, agent)\n",
    "plot_rewards(res_dic['rewards'], title=f\"testing curve on {cfg.device} of {cfg.algo_name} for {cfg.env_name}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
